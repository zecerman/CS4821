{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "591ab9e9-5501-431d-9296-9f9db1b5ed9a",
   "metadata": {},
   "source": [
    "# a3 - Python\n",
    "\n",
    "This assignment will cover topics of classification and dimensionality reduction.\n",
    "\n",
    "Make sure that you keep this notebook named as \"a3-4821.ipynb\" \n",
    "\n",
    "Any other packages or tools, outside those listed in the assignments or Canvas, should be cleared\n",
    "by Dr. Brown before use in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b64615-6e16-4c36-a3e5-9ebd4fd53bc8",
   "metadata": {},
   "source": [
    "# Q0 - Setup\n",
    "\n",
    "The following code looks to see whether your notebook is run on Gradescope (GS), Colab (COLAB), or the linux Python environment you were asked to setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46b79a14-abb4-482a-a14a-f2e68c24177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: GS - False, COLAB - False, LLM - True\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import os\n",
    "import platform \n",
    "import sys \n",
    "\n",
    "# flag if notebook is running on Gradescope \n",
    "if re.search(r'amzn', platform.uname().release): \n",
    "    GS = True\n",
    "else: \n",
    "    GS = False\n",
    "\n",
    "# flag if notebook is running on Colaboratory \n",
    "try:\n",
    "  import google.colab\n",
    "  COLAB = True\n",
    "except:\n",
    "  COLAB = False\n",
    "\n",
    "# flag if running on Linux lab machines. \n",
    "cname = platform.uname().node\n",
    "if re.search(r'(guardian|colossus|c28|coc-15954-m)', cname):\n",
    "    LLM = True \n",
    "else: \n",
    "    LLM = False\n",
    "\n",
    "print(\"System: GS - %s, COLAB - %s, LLM - %s\" % (GS, COLAB, LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93ae8298-76c6-444d-9404-b25c41e67abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard DS packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy\n",
    "import statistics\n",
    "import textwrap\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree        # decision tree classifier\n",
    "from sklearn import neighbors   # knn classifier\n",
    "from sklearn import naive_bayes # naive bayes classifier \n",
    "from sklearn import svm         # svm classifier\n",
    "from sklearn import ensemble    # ensemble classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import metrics     # performance evaluation metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Package for Autograder \n",
    "import os\n",
    "if os.environ[\"HOME\"]=='/home/jovyan':\n",
    "    !pip install --upgrade otter-grader\n",
    "    \n",
    "import otter \n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec83cbf-7761-460f-9ee1-23ee10665f1b",
   "metadata": {},
   "source": [
    "# Q1 - NBA Rookies\n",
    "\n",
    "For this problem, you will go back to using the data set of rookie NBA players from 1980 - 2017 seasons.  The dataset was collected from the NBA website API - https://www.nba.com. \n",
    "\n",
    "You will use the data from their rookie year to predict whether a player will last at least 5 seasons in the leaque. \n",
    "\n",
    "The data consists of variables:\n",
    "\n",
    "* `PlayerID`, `Player` - variables to identify individual samples (ignore for prediction)\n",
    "* `Tm`, `Year` - variables describing the year the player started and for what team (ignore for prediction)  \n",
    "* `TARGET` - This is the target / class feature to be predicted (whether the player was in the league for at least 5 years). \n",
    "\n",
    "The remaining variables are predictor variables for the models.  They come in pairs \"*\\_DIFF\" and \"*\\_A\" reporting the given statistic as the difference between Team A and Team B and the statistic itself for Team A. \n",
    "\n",
    "* `Pos` - position of the player, power forward, point guard, shooting guard, center, etc.\n",
    "* `Age` - player age\n",
    "* `G` - sum of number of games played\n",
    "* `GS` - sum of number of games started\n",
    "* `MP` - sum of number of minutes played\n",
    "* `PTS` - sum of number of points scored\n",
    "* `FG` - sum of number of field goals made (both 2 and 3 pointers)\n",
    "* `FGA` - sum of number of field goals attempted\n",
    "* `FG%` - FG / FGA, percentage of field goals made \n",
    "* `3P` - sum of the number of 3 pointers made\n",
    "* `3PA` - sum of the number of 3 pointers attempted\n",
    "* `3P%` - 3P / 3PA, percentage of 3 pointers made\n",
    "* `2P` - sum of the number of 2 point shots made\n",
    "* `2PA` - sum of the number of 2 point shots attempted\n",
    "* `2P%` - 2P / 2PA, percentage of 2 point shots made\n",
    "* `eFG%` - Effective Field Goal Percentage, (`FG` + 0.5 * `3P`)/`FGA`\n",
    "* `FT` - sum of the number of free throws made\n",
    "* `FTA` - sum of the number of free throws attempted\n",
    "* `FT%` - FT / FTA, percentage of free throws made\n",
    "* `ORB` - sum of the number of offensive rebounds\n",
    "* `DRB` - sum of the number of defensive rebounds\n",
    "* `TRB` - sum of the number of total rebounds\n",
    "* `AST` - sum of the number of assists\n",
    "* `STL` - sum of the number of steals\n",
    "* `BLK` - sum of the number of blocks\n",
    "* `TOV` - sum of the number of turnovers\n",
    "* `PF` - sum of the number of personal fouls\n",
    "\n",
    "More information on the stats used can be found: https://www.nba.com/stats/help/glossary   \n",
    "*Note, some of the abbreviations used here are slightly different* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d3bfb-b89a-420d-9a5f-8305c40efe48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## Q1(a) - Load Data, Handle Missing Data\n",
    "\n",
    "**Same process as Q6(a) and Q6(c) on Assignment a2**\n",
    "\n",
    "Load the `nba` data.\n",
    "\n",
    "Next, you must handle the missing data. \n",
    "\n",
    "First, the missing values for GS, because there is not way to impute these values (and there are so few players in the 1980 and 1981 season having this information), the all samples from these two seasons should be deleted.\n",
    "\n",
    "For the missing values in the \"percentage\" columns, replace those missing values with 0.\n",
    "\n",
    "Call you new DataFrame after performing these operations nba2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad488a79-00a7-4432-9a93-02fa3989f5a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PlayerID</th>\n",
       "      <th>Player</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tm</th>\n",
       "      <th>Year</th>\n",
       "      <th>G</th>\n",
       "      <th>GS</th>\n",
       "      <th>MP</th>\n",
       "      <th>PTS</th>\n",
       "      <th>...</th>\n",
       "      <th>FT%</th>\n",
       "      <th>ORB</th>\n",
       "      <th>DRB</th>\n",
       "      <th>TRB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abdelal01</td>\n",
       "      <td>Alaa Abdelnaby</td>\n",
       "      <td>PF</td>\n",
       "      <td>22</td>\n",
       "      <td>POR</td>\n",
       "      <td>1991</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abdulma02</td>\n",
       "      <td>Mahmoud Abdul-Rauf</td>\n",
       "      <td>PG</td>\n",
       "      <td>21</td>\n",
       "      <td>DEN</td>\n",
       "      <td>1991</td>\n",
       "      <td>67</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1505</td>\n",
       "      <td>942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>34</td>\n",
       "      <td>87</td>\n",
       "      <td>121</td>\n",
       "      <td>206</td>\n",
       "      <td>55</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>149</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdulta01</td>\n",
       "      <td>Tariq Abdul-Wahad</td>\n",
       "      <td>SG</td>\n",
       "      <td>23</td>\n",
       "      <td>SAC</td>\n",
       "      <td>1998</td>\n",
       "      <td>59</td>\n",
       "      <td>16.0</td>\n",
       "      <td>959</td>\n",
       "      <td>376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>44</td>\n",
       "      <td>72</td>\n",
       "      <td>116</td>\n",
       "      <td>51</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>65</td>\n",
       "      <td>81</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abdursh01</td>\n",
       "      <td>Shareef Abdur-Rahim</td>\n",
       "      <td>PF</td>\n",
       "      <td>20</td>\n",
       "      <td>VAN</td>\n",
       "      <td>1997</td>\n",
       "      <td>80</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2802</td>\n",
       "      <td>1494</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745665</td>\n",
       "      <td>216</td>\n",
       "      <td>339</td>\n",
       "      <td>555</td>\n",
       "      <td>175</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>225</td>\n",
       "      <td>199</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abrinal01</td>\n",
       "      <td>Álex Abrines</td>\n",
       "      <td>SG</td>\n",
       "      <td>23</td>\n",
       "      <td>OKC</td>\n",
       "      <td>2017</td>\n",
       "      <td>68</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1055</td>\n",
       "      <td>406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>18</td>\n",
       "      <td>68</td>\n",
       "      <td>86</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>114</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PlayerID               Player Pos  Age   Tm  Year   G    GS    MP   PTS  \\\n",
       "0  abdelal01       Alaa Abdelnaby  PF   22  POR  1991  43   0.0   290   135   \n",
       "1  abdulma02   Mahmoud Abdul-Rauf  PG   21  DEN  1991  67  19.0  1505   942   \n",
       "2  abdulta01    Tariq Abdul-Wahad  SG   23  SAC  1998  59  16.0   959   376   \n",
       "3  abdursh01  Shareef Abdur-Rahim  PF   20  VAN  1997  80  71.0  2802  1494   \n",
       "4  abrinal01         Álex Abrines  SG   23  OKC  2017  68   6.0  1055   406   \n",
       "\n",
       "   ...       FT%  ORB  DRB  TRB  AST  STL  BLK  TOV   PF  TARGET  \n",
       "0  ...  0.568182   27   62   89   12    4   12   22   39    True  \n",
       "1  ...  0.857143   34   87  121  206   55    4  110  149    True  \n",
       "2  ...  0.672000   44   72  116   51   35   13   65   81    True  \n",
       "3  ...  0.745665  216  339  555  175   79   79  225  199    True  \n",
       "4  ...  0.897959   18   68   86   40   37    8   33  114   False  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nba = pd.read_csv('nba_rookies.csv', na_values=[\"N/A\", \"NA\", \" \", \"\"]) \n",
    "\n",
    "nba2 = nba[~nba['Year'].isin([1980, 1981])].fillna(0)  # nba data with missing values handles. \n",
    "\n",
    "nba2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8530036",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1a</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q1a results: All test cases passed!"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126a903-150d-4133-b545-858f2e025af2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(b) - Prepare the data \n",
    "\n",
    "**Same process as Q6(e) on Assignment a2**\n",
    "\n",
    "At this point, we need to set up the data in order to be used in the classification models. \n",
    "\n",
    "We want to create a DataFrame `nbaX` for the predictor variables and `nbaY` for the target variable. \n",
    "\n",
    "The target variable, `nbaY` will just be the `Target` column of the `nba2` data set. \n",
    "\n",
    "The predictor variables have more considerations. \n",
    "\n",
    "We want to exclude player information such as IDs, `PlayerID` and names `Player`, that are identifying and not predictive.   \n",
    "\n",
    "You should also exclude these other factors (note, that some of these variables may in fact be predictive but we are going to exclude at this time): \n",
    "\n",
    "* `POS` - player position\n",
    "* `Tm` - team\n",
    "* `Year` - rookie year\n",
    "* `Age` - player age\n",
    "\n",
    "Finally, several of the numeric predictive variables are not only related, but can be directly calculated from one another, e.g., `FG%` = `FG` / `FGA`.  Having variables that are closely related, or in this case redundant may actually hinder the predictive models. \n",
    "\n",
    "Therefore, exclude the following variables: `FG`, `3P`, `2P`, `eFG%`, `FT`, `TRB`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0e3f30d-5d01-4512-be49-b96ca2bd3e65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2487, 19)\n",
      "(2487,)\n"
     ]
    }
   ],
   "source": [
    "nbaY = nba2['TARGET']\n",
    "nbaX = nba2.drop(columns=['TARGET','PlayerID','Player','Pos','Tm','Year','FG','3P','2P','eFG%','FT','TRB','Age'])\n",
    "\n",
    "print(nbaX.shape)\n",
    "print(nbaY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7fd7cdb9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1b</pre></strong> passed! 🎉</p>"
      ],
      "text/plain": [
       "q1b results: All test cases passed!"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbde3cf0-8b50-44c9-a753-a10314ac5a8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(c) - Model Selection and Evaluation: Three-fold Split and Scaling\n",
    "\n",
    "**Same process as Q6(f) on Assignment a2**\n",
    "\n",
    "\n",
    "\n",
    "Split the data into training, validation and test sets with 60, 20, and 20% of the data respectively. Make sure to split the data such that the distribution of class labels is approximately equal across splits - “stratify”.\n",
    "\n",
    "Set the seed for the random generator in `random_state` to ”4821”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "434cdef7-a86b-460e-a852-2bff52aad125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split of the test set\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(nbaX, nbaY, test_size=0.2, random_state=4821, stratify=nbaY)\n",
    "\n",
    "# Split trainval into train + val \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=4821, stratify=y_trainval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89496ae9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1c</pre></strong> passed! 💯</p>"
      ],
      "text/plain": [
       "q1c results: All test cases passed!"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fcce7d-a846-4b9a-b781-1142e82047e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(d) - Support Vector Machines + GridSearch with Cross-validation (without using GridSearchCV) \n",
    "\n",
    "In this part, you will use the *do-it-yourself* approach using `StratifiedKFold` (rather than GridSearchCV). \n",
    "\n",
    "Use the same split from above with 80% train+val, 20% test data. \n",
    "\n",
    "With the train+val data, use 10-fold cross-validation (make sure to use Stratified approach with random_state = 4821).  Train each model on the training set and evaluate each model on the validation set.  \n",
    "\n",
    "*Consider how to do standard scaling with this approach.  That is, think about where you fit the parameters for scaling and then transform various parts of the data set.*  \n",
    "\n",
    "You will consider SVM models with the following hyperparameters: \n",
    "\n",
    "* Polynomial kernel with C = [10^-2, 10^-1, 1], degree = [1, 2, 3], with `coef0 = 1`\n",
    "* RBF kernel (Gaussian kernel) with C = [10^-2, 10^-1, 1] \n",
    "\n",
    "Collect each model's validation performance (AUC) in `svm_poly_auc_val` and `svm_rbf_auc_val` variables.  \n",
    "\n",
    "Report the mean validation performance (AUC) as DataFrame with: \n",
    "\n",
    "- rows, Linear kernel, poly kernel d=2, poly kernel d=3, rbf kernel \n",
    "- columns, C = [10^-2, 10^-1, 1]\n",
    "\n",
    "Report the best parameter combination (cost + kernel). \n",
    "\n",
    "Retrain the best model on train+val and report the test performance.  *Make sure to scale the train+val and test data, like it was done in Q6(g) in Assignment a2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc03712a-96ed-4de4-93e7-ac8ecd3f436b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best hyperparameters: 1.0, poly, 3\n",
      "SVM iloc[0,0] = 0.7225555086120856, 0.7306011835877465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C=0.01</th>\n",
       "      <th>C=0.1</th>\n",
       "      <th>C=1</th>\n",
       "      <th>Kernel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.722556</td>\n",
       "      <td>0.731187</td>\n",
       "      <td>0.730637</td>\n",
       "      <td>Linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.730601</td>\n",
       "      <td>0.734455</td>\n",
       "      <td>0.732879</td>\n",
       "      <td>Poly d=2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.734758</td>\n",
       "      <td>0.735173</td>\n",
       "      <td>0.740830</td>\n",
       "      <td>Poly d=3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725388</td>\n",
       "      <td>0.734733</td>\n",
       "      <td>0.735814</td>\n",
       "      <td>RBF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C=0.01     C=0.1       C=1    Kernel\n",
       "0  0.722556  0.731187  0.730637    Linear\n",
       "1  0.730601  0.734455  0.732879  Poly d=2\n",
       "2  0.734758  0.735173  0.740830  Poly d=3\n",
       "3  0.725388  0.734733  0.735814       RBF"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "svm_poly_auc_val = np.zeros((10,3,3))\n",
    "svm_rbf_auc_val = np.zeros((10,3))\n",
    "\n",
    "\n",
    "# With the train+val data, use 10-fold cross-validation (with StratifiedKFold )  \n",
    "# Train each model on the train set, evaluate each model on the validation set\n",
    "#  set up scaling \n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "\n",
    "# You will consider SVM models with the following hyperparameters: \n",
    "#   - Polynomial kernel with C = [10^-2, 10^-1, 1], degree = [1, 2, 3]\n",
    "#   - RBF kernel (Gaussian kernel) with C = [10^-2, 10^-1, 1] \n",
    "Cs = [0.01, 0.1, 1.0]\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "# Collect each model's validation performance. \n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=4821)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_trainval, y_trainval)):\n",
    "    X_train_fold, X_val_fold = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_trainval.values[train_idx], y_trainval.values[val_idx]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_fold_sc = scaler.fit_transform(X_train_fold)\n",
    "    X_val_fold_sc = scaler.transform(X_val_fold)\n",
    "    \n",
    "    for i, C in enumerate(Cs):\n",
    "        for j, degree in enumerate(degrees):\n",
    "            svm_poly = svm.SVC(kernel='poly', C=C, degree=degree, coef0=1.0)\n",
    "            svm_poly.fit(X_train_fold_sc, y_train_fold)\n",
    "            y_val_pred = svm_poly.predict(X_val_fold_sc)\n",
    "            svm_poly_auc_val[fold, i, j] = metrics.roc_auc_score(y_val_fold, y_val_pred) \n",
    "    for i, C in enumerate(Cs):\n",
    "        svm_rbf = svm.SVC(kernel='rbf', C=C)\n",
    "        svm_rbf.fit(X_train_fold_sc, y_train_fold)\n",
    "        y_val_pred = svm_rbf.predict(X_val_fold_sc)\n",
    "        svm_rbf_auc_val[fold, i] = metrics.roc_auc_score(y_val_fold, y_val_pred)\n",
    "\n",
    "\n",
    "# Report the mean validation performance (AUC) as DataFrame with: \n",
    "#   - rows, Linear kernel, poly kernel d=2, poly kernel d=3, rbf kernel \n",
    "#   - columns, C = [10^-2, 10^-1, 1]\n",
    "mean_poly_auc = np.mean(svm_poly_auc_val, axis=0)\n",
    "mean_rbf_auc = np.mean(svm_rbf_auc_val, axis=0)\n",
    "svm_results = pd.DataFrame(\n",
    "    {\n",
    "        'C=0.01': [\n",
    "            mean_poly_auc[0, 0],  # Linear kernel, C=0.01\n",
    "            mean_poly_auc[0, 1],  # Poly d=2, C=0.01\n",
    "            mean_poly_auc[0, 2],  # Poly d=3, C=0.01\n",
    "            mean_rbf_auc[0]       # RBF kernel, C=0.01\n",
    "        ],\n",
    "        'C=0.1': [\n",
    "            mean_poly_auc[1, 0],  # Linear kernel, C=0.1\n",
    "            mean_poly_auc[1, 1],  # Poly d=2, C=0.1\n",
    "            mean_poly_auc[1, 2],  # Poly d=3, C=0.1\n",
    "            mean_rbf_auc[1]       # RBF kernel, C=0.1\n",
    "        ],\n",
    "        'C=1': [\n",
    "            mean_poly_auc[2, 0],  # Linear kernel, C=1\n",
    "            mean_poly_auc[2, 1],  # Poly d=2, C=1\n",
    "            mean_poly_auc[2, 2],  # Poly d=3, C=1\n",
    "            mean_rbf_auc[2]       # RBF kernel, C=1\n",
    "        ],\n",
    "        'Kernel': ['Linear', 'Poly d=2', 'Poly d=3', 'RBF'],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Report the best hyperparameter combination (cost + kernel). \n",
    "best_poly_idx = np.unravel_index(np.argmax(mean_poly_auc), mean_poly_auc.shape)\n",
    "best_rbf_idx = np.argmax(mean_rbf_auc)\n",
    "\n",
    "if mean_poly_auc[best_poly_idx] > mean_rbf_auc[best_rbf_idx]:\n",
    "    svm_bestC = Cs[best_poly_idx[0]]\n",
    "    svm_bestKernel = 'poly'\n",
    "    svm_bestD = degrees[best_poly_idx[1]]\n",
    "else:\n",
    "    svm_bestC = Cs[best_rbf_idx]\n",
    "    svm_bestKernel = 'rbf'\n",
    "    svm_bestD = np.nan\n",
    "\n",
    "# Retrain the best model on train+val and report the test performance .\n",
    "#  Scale train+val and test data like Q6(g) in Assignment a2 .\n",
    "scaler_final = StandardScaler()\n",
    "X_trainval_sc = scaler_final.fit_transform(X_trainval)\n",
    "X_test_sc = scaler_final.transform(X_test)\n",
    "\n",
    "if svm_bestKernel == 'poly':\n",
    "    best_model = svm.SVC(kernel='poly', C=svm_bestC, degree=svm_bestDegrees, coef0=1.0)\n",
    "else:\n",
    "    best_model = svm.SVC(kernel='rbf', C=svm_bestC)\n",
    "    \n",
    "best_model.fit(X_trainval_sc, y_trainval)\n",
    "y_test_pred = best_model.predict(X_test_sc)\n",
    "svm_auc_test = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'best hyperparameters: {svm_bestC}, {svm_bestKernel}, {svm_bestD}')\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccaf63ca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1d</pre></strong> passed! 💯</p>"
      ],
      "text/plain": [
       "q1d results: All test cases passed!"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c7727ae-f571-4a38-8d26-ce5880dad0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7225555086120856)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_results.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a023a2-a997-4b0f-b494-f66383250a68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(e) -  Ensemble Methods + GridSearchCV with Pipelines\n",
    "\n",
    "Let’s examine bagging & boosting ensemble approaches for prediction.\n",
    "\n",
    "For this part, we will use the **preferred method for building predictor models by using pipelines**. \n",
    "\n",
    "You will create a pipeline for the Random Forest models and a pipeline for the AdaBoost models. Both pipelines will use standard scaling preprocessing.  Label the steps of the pipeline with \"sc\" for the scaling step and \"rf\" and \"ab\" for the modeling step in the two different pipelines.\n",
    "\n",
    "For the random forest, consider hyper-parameters for the maximum number of features: [2, 4, 8, 16] and number of estimators of [25, 50, 100].  For AdaBoost, consider the hyper-parameter of the number of estimators as [10, 25, 50, 100]. \n",
    "\n",
    "To ensure repeatability or your code (and to compare to the autograder) make sure to set the random state in both classifiers and the stratified 10-fold cross-validation to \"4821\".  \n",
    "\n",
    "Use AUC as the scoring metric for the GridSearch criteria.  \n",
    "\n",
    "You will need to report the best hyper-parameters for both models as well as the final test set performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a420b-e293-4903-b24b-b40d1c3fcc4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# You will create a pipeline for both the Random Forest and AdaBoost models. \n",
    "#   Both pipelines will use standard scaling preprocessing. \n",
    "#   Call the scaling step \"sc\" and model step for each pipeline, \"rf\" / \"ab\" \n",
    "#   Set the random state in both classifiers to 4821\n",
    "ab_pipe = ...\n",
    "\n",
    "rf_pipe = ... \n",
    "\n",
    "# RF: hyper-parameters for the maximum number of features: [2, 4, 8, 16] and \n",
    "#   number of estimators of [25, 50, 100].  \n",
    "rf_params = ...\n",
    "\n",
    "# AdaBoost: hyper-parameter of the number of estimators as [10, 25, 50, 100].\n",
    "ab_params = ...\n",
    "\n",
    "# Set the random state in the stratified k-fold cv to 4821  \n",
    "cvStrat = ... \n",
    "\n",
    "# Use AUC as the scoring metric for the GridSearch criteria. \n",
    "rf_grid = ... \n",
    "\n",
    "ab_grid = ...\n",
    "\n",
    "# Report the best hyper-parameters and final test set performance \n",
    "rf_best_params = ...\n",
    "ab_best_params = ...\n",
    "\n",
    "\n",
    "rf_auc_test = ...\n",
    "ab_auc_test = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf936ecd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10cb70-5c3f-4660-bf51-2164ac64502d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q1(f) - Feature Selection \n",
    "\n",
    "Now that you have explored several different classification models and how to create pipelines, let's explore create a pipeline that considers different scaling methods, feature selection options, and models in a pipeline. \n",
    "\n",
    "Specifically, you will create a pipeline that considers the following steps using the labels below. \n",
    "\n",
    "* Scaling - `scaler`\n",
    "    * MinMaxScaling with default values\n",
    "    * StandardScaling with the default values\n",
    "* Feature Selection - `feat_sel`\n",
    "    * Use `SelectPercentile` as a way to select features\n",
    "    * For the `SelectPercentile` consider 10, 25, 50, 75, and 100% of your data, use the `f_classif` option to select the features\n",
    "* Classification Models - `classify`\n",
    "    * Consider a KNN model with k = [11, 15, 19]\n",
    "    * Consider a Random Forest model with number of trees [10, 25, 50] and max_features in [2, 8, 16]\n",
    "    * Consider an AdaBoost model with number of estimators of [10, 25, 50]\n",
    " \n",
    "To ensure repeatability or your code (and to compare to the autograder) make sure to set the random state in the Random Forest and AdaBoost classifiers and the stratified 10-fold cross-validation to \"4821\".  \n",
    "\n",
    "Use AUC as the scoring metric for the GridSearch criteria.  \n",
    "\n",
    "You will need to report the best hyper-parameters as well as the final test set performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d327a-96f9-419c-9298-a10e59a710c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#  you will create a pipeline that considers the following steps\n",
    "#    - Scaling - labeled `scaler`\n",
    "#    - Feature Selection - labeled `feat_sel`\n",
    "#    - Classification model - labeled `classify`\n",
    "\n",
    "final_pipe = ... \n",
    "\n",
    "# Consider the following hyperparameters for each step:\n",
    "#  (you may need to create a list of dictionaries to store the options) \n",
    "# * Scaling - `scaler`\n",
    "#     * MinMaxScaling with default values\n",
    "#     * StandardScaling with the default values\n",
    "# * Feature Selection - `feat_sel`\n",
    "#     * Use `SelectPercentile` as a way to select features\n",
    "#     * For the `SelectPercentile` consider 10, 25, 50, 75, and 100% of the data\n",
    "#         use the `f_classif` option to select the features\n",
    "# * Classification Models - `classify`\n",
    "#     * Consider a KNN model with k = [11, 15, 19]\n",
    "#     * Consider a Random Forest model with number of trees [10, 25, 50] and \n",
    "#          max_features in [2, 5, 10, 15]\n",
    "#     * Consider an AdaBoost model with number of estimators of [10, 25, 50]\n",
    "\n",
    "final_params = ...\n",
    "\n",
    "\n",
    "# Set the random state in the stratified k-fold cv to 4821  \n",
    "cvStrat = ... \n",
    "\n",
    "# Use AUC as the scoring metric for the GridSearch criteria. \n",
    "final_grid = ... \n",
    "\n",
    "\n",
    "# Report the best hyper-parameters and final test set performance \n",
    "final_best_params = ...\n",
    "\n",
    "final_auc_test = ...\n",
    "\n",
    "print(final_best_params)\n",
    "print(final_auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d3be3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a450ca0-5609-49bc-b9fa-9b854154a6e1",
   "metadata": {},
   "source": [
    "# Q2 - Pokemon Data \n",
    "\n",
    "Consider the following data set of Pokemon (collected from several sources, pokemon.com, pokemondb, bulbapedia). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6781b30-756e-4fe4-84eb-fe530dc6bb82",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q2(a) - Load the data \n",
    "\n",
    "Load the Pokemon data into a Data Frame, `pok`, consider how to read in missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a0082-6ff0-4835-b204-d1ca723a5ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in Pokemon data\n",
    "\n",
    "pok = pd.read_csv(.....)\n",
    "\n",
    "pok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73d524",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c306c-722e-43e8-8fbc-52d2feef5a14",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2(b) - Explore the Data \n",
    "\n",
    "Look at the distribution of Pokemon skills we will use in this analysis: `hp`, `attack`, `defense`, `sp_attack` - Special Attack, `sp_defense` - Special Defense, and `speed`. \n",
    "\n",
    "Create a violinplot showing the distribution of these variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e424c5d-7190-450f-a63c-65435822e155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create violinplot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403839b-4f0a-4611-9fbc-98f8fbec1a8e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Q2(c) - Prepare the data \n",
    "\n",
    "Let's use the 6 pokemon skills viewed above.  \n",
    "\n",
    "The six features (Pokemon Skills) have different ranges, therefore we should scale the data before considering PCA. Standardize the data over the six features of interest, store only these variables in `pokScaled` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c45da-bc26-4954-9d68-6420ce5b2194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale the data \n",
    "\n",
    "pokScaled = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10ac16",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a963297-4a9a-479d-9a1d-123bba98f67f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q2(d) - Principal Components Analysis \n",
    "\n",
    "Perform principal components analysis (PCA) on the scaled Pokemon skills data. \n",
    "Store the results of PCA in `pokPCA` and the transformed data in `pokTr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e5614-eba7-4469-9e5e-0befe297b506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run PCA on the scaled data \n",
    "\n",
    "pokPCA = ...\n",
    "pokTr = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c31d54",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e5fbac-7413-47db-a658-ed2423536381",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2(e) - Explore the Results \n",
    "\n",
    "Plot the transformed data in the space defined by the first two principal components. This should be a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef0bb4-ce7e-4609-b11d-96f50dcc076c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the transformed data  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c811b-665d-4c44-9c92-60a164f14f44",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2(f) - Explore the Results, part 2 \n",
    "\n",
    "Explore the amount of variance explained by each principal component direction. Plot the proportion of variance explained (y-axis) vs. the different principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a37673-c20f-4c0b-a5d6-bea29b5920c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot variance explained by each component   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb7202-b688-4b40-a2fb-a85a3997f588",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2(g) - Explore the Results, part 3\n",
    "\n",
    "Plot the cumulative proportion of variance explained by the principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cd6cc-f377-40ee-aade-d439f349a6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the cumulative variance explained.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ae03a-cb3f-40bd-be0f-8fcd0c1cf4e4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q2 bonus \n",
    "\n",
    "Display the transformed data in the space defined by the first two principal components.  Let's also color the data points by `type1`.  Place the legend outside the plot to the right. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c04d8-a0ac-4e1f-a234-599fb7c4c48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create plot   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f84958-1d00-43e1-b962-081e6993e7a0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "# Q3 - Polarization of Congress \n",
    "\n",
    "For this part of the project you will consider methods to group members of U.S. House of Representatives based on their voting records. The voting records from congress are available at Office of the Clerk, US House of Representatives, but not in a form that is easily digestible for analysis.\n",
    "\n",
    "In fact it was only in 2016, that Congress agreed to make legislative data available themselves. Govtrack.us has links to primary data sources and api’s projects that collect and release the data in easier digestible forms.\n",
    "\n",
    "A long-standing project to document congressional roll call votes at the Interuniversity Consortium for Political and Social Research (ICPSR). This data includes roll call votes from 1789 - 1990. The ICPSR formatting for storing this data has been used on other sites which are keeping up with the creating a record.\n",
    "\n",
    "The data you will use was downloaded in its raw state from https://voteview.com/data. Then, for each of the last 20 congresses (up until 2021), 50 random votes were selected for each member (ignoring the first 10 votes of each session - these votes usually have to do with electing a Speaker of the House and rules votes). The results are stored in the the files `H97_votes.csv`, `H98_votes.csv`, ..., `H116_votes.csv`.\n",
    "\n",
    "\n",
    "References: \n",
    "- http://clerk.house.gov/legislative/legvotes.aspx\n",
    "- https://www.icpsr.umich.edu/icpsrweb/ICPSR/series/159"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96aff1-54a2-44d6-860a-b23339c3f859",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(a) - Load and prepare the data \n",
    "\n",
    "Working with data for the 116th Congress, H116 votes.csv, read in the vote data. \n",
    "\n",
    "Preprocess the data appropriately. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34a6b9-5180-42cb-8ea5-84c27d4fca6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in votes data into DataFrame \n",
    "\n",
    "votes = ...\n",
    "\n",
    "# prepare the data for PCA \n",
    "# Scaled votes data only\n",
    "votes_sc = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7ecf8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385c127-9925-4fe5-95e6-166666cd6c01",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Q3(b) - PCA \n",
    "\n",
    "Perform principal component analysis on the voting records.  \n",
    "\n",
    "Plot the first two principal components. Because we know the party affiliation of each member of congress, color the plot based on party (red = Republican, PartyCode=200; blue = Democrat, PartyCode = 100; green = Independent, PartyCode in 300s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce89365-9a5a-41f4-8f8b-dc76e4bb90a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run PCA and transform the data \n",
    "\n",
    "votes_pca = ...\n",
    "votes_tr = ...\n",
    "\n",
    "# Create plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14379bb8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60a4aa-7133-44b6-9c04-46051b5f3528",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Q3(c) - Historical Polarization (bonus)\n",
    "\n",
    "Create a small multiples plot (6 x 3) showing the results of PCA (colored by party) for the 18 Congresses starting with the 97th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c17db-b138-486f-b47b-1e1812723679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create small multiples plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1250497",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34c4cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "**NOTE** the submission must be run on the campus linux machines.  See the instruction in the Canvas assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765058fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7229ab5",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4821",
   "language": "python",
   "name": "cs4821"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "a3-4821",
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nba.shape == (2621, 32)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nba.columns == ['PlayerID', 'Player', 'Pos', 'Age', 'Tm', 'Year', 'G', 'GS', 'MP', 'PTS', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'TARGET'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nba['TARGET'].value_counts() == [1315, 1306])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> nba2.shape == (2487, 32)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> nba2.isna().sum().sum() == 0\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> any(nba2['Year'].isin([1980, 1981])) == False\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> nbaX.shape == (2487, 19)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> nbaY.shape == (2487,)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nba.columns == ['PlayerID', 'Player', 'Pos', 'Age', 'Tm', 'Year', 'G', 'GS', 'MP', 'PTS', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'TARGET'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(nbaY.value_counts() == [1258, 1229])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_train.shape == (1491, 19) and y_train.shape == (1491,)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_val.shape == (498, 19) and y_val.shape == (498,)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> X_test.shape == (498, 19) and y_test.shape == (498,)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": [
      1.5,
      1.5,
      1.5,
      1.5,
      1.5,
      1.5
     ],
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(svm_results.iloc[0, 0], 0.722556) and np.isclose(svm_results.iloc[1, 0], 0.730601)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(svm_results.iloc[2, 0], 0.734758) and np.isclose(svm_results.iloc[3, 0], 0.725388)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(svm_results.iloc[0, 1], 0.731187) and np.isclose(svm_results.iloc[1, 1], 0.734455)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(svm_results.iloc[2, 1], 0.735173) and np.isclose(svm_results.iloc[3, 1], 0.734733)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(svm_results.iloc[0, 2], 0.730637) and np.isclose(svm_results.iloc[1, 2], 0.732879)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(svm_results.iloc[2, 2], 0.74083) and np.isclose(svm_results.iloc[3, 2], 0.735814)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(rf_auc_test, 0.7476771)\nnp.True_",
         "hidden": false,
         "locked": false,
         "points": 3
        },
        {
         "code": ">>> rf_best_params['rf__max_features'] == 2\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1.5
        },
        {
         "code": ">>> rf_best_params['rf__n_estimators'] == 100\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1.5
        },
        {
         "code": ">>> rf_pipe.get_params()['rf__random_state'] == 4821\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": 15,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(final_auc_test, 0.75391986)\nnp.True_",
         "hidden": false,
         "locked": false,
         "points": 2.5
        },
        {
         "code": ">>> str(final_best_params['scaler']).startswith('MinMax')\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> pok.shape == (801, 41)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(pok.columns == ['abilities', 'against_bug', 'against_dark', 'against_dragon', 'against_electric', 'against_fairy', 'against_fight', 'against_fire', 'against_flying', 'against_ghost', 'against_grass', 'against_ground', 'against_ice', 'against_normal', 'against_poison', 'against_psychic', 'against_rock', 'against_steel', 'against_water', 'attack', 'base_egg_steps', 'base_happiness', 'base_total', 'capture_rate', 'classfication', 'defense', 'experience_growth', 'height_m', 'hp', 'japanese_name', 'name', 'percentage_male', 'pokedex_number', 'sp_attack', 'sp_defense', 'speed', 'type1', 'type2', 'weight_kg', 'generation', 'is_legendary'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(pok.iloc[0, 1:8] == [1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 2.0])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(pok.iloc[10:15, 38] == [9.9, 32.0, 3.2, 10.0, 29.5])\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> pokScaled.shape == (801, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> type(pokScaled) == np.ndarray\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokScaled[0, 1:4], [-0.89790944, -0.78077335, -0.19502508]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokScaled[10:14, 5], [-1.25770406, 0.12687669, -0.56541369, -1.08463147]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokScaled[598:602, 3], [-0.81357622, -0.0403873, -0.0403873, -0.81357622]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokScaled[764, 1:5], [-0.55564338, 0.22735847, 0.57816384, 1.39976953]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2d": {
     "name": "q2d",
     "points": 8,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> pokPCA.components_.shape == (6, 6)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokPCA.singular_values_, [46.37717271, 29.59944842, 25.13525664, 24.28527527, 18.56182332, 14.59224105]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokPCA.explained_variance_, [2.68855269, 1.09515918, 0.78972641, 0.73721824, 0.43067661, 0.26616687]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pokPCA.n_components_ == 6\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pokPCA.n_features_in_ == 6\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokPCA.components_[1, 1:4], [0.00758374, 0.59596455, -0.30342622]))\nFalse",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(pokPCA.mean_, [-7.98362624e-17, 2.04026004e-16, -1.46366481e-16, -1.99590656e-16, 1.68543221e-16, 4.43534791e-18]))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> pokPCA.n_samples_ == 801\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> votes.shape == (439, 58)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> votes_sc.shape == (439, 50)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(np.isclose(votes_sc[4:6, 3:7], np.array([[0.96619664, 1.03995153, -0.18356312, -0.17495795], [0.96619664, 1.03995153, -0.18356312, -0.17495795]])).flatten())\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> votes_pca.singular_values_.shape == (50,)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
